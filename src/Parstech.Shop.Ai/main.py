import pandas as pd
from fastapi import FastAPI
import logging
import pyodbc
import sys
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯ÛŒÙ†Ú¯
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

app = FastAPI()

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ ---
MODEL_NAME = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'
DB_CONN_STR = (
    'DRIVER={ODBC Driver 17 for SQL Server};'
    'SERVER=localhost;'
    'DATABASE=parstech;'
    'Trusted_Connection=yes;'
)
CHROMA_DIR = "./chroma_db"

# --- Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø³Ø±Ø§Ø³Ø±ÛŒ ---
chroma_client = None
collection = None
products_df = None
model = None
conn = None  # Ù…Ø¯ÛŒØ±ÛŒØª Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³

# --- ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ ---
def load_data_and_init_chroma():
    global products_df, chroma_client, collection, model, conn
    
    try:
        # 1. Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³
        logging.info("ğŸ“¡ Ø¯Ø± Ø­Ø§Ù„ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³...")
        conn = pyodbc.connect(DB_CONN_STR)
        
        # 2. Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª
        logging.info("ğŸ” Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³...")
        products_df = pd.read_sql("""
            SELECT
                Id,
                Name,
                COALESCE(Description, '') AS Description,
                COALESCE(Keywords, '') AS Keywords,
                IsActive
            FROM Product
            WHERE IsActive = 1
        """, conn)

        # 3. ØªØ±Ú©ÛŒØ¨ Ù…ØªÙ† Ø¨Ø§ ÙˆØ²Ù† Ø¯Ù‡ÛŒ Ø¨Ù‡ Ù†Ø§Ù… Ù…Ø­ØµÙˆÙ„
        products_df['combined_text'] = (
            products_df['Name'] + " " +
            products_df['Description'].str[:200] + " " +  # Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø·ÙˆÙ„
            products_df['Keywords']
        )
                
        # 4. Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ ChromaDB
        logging.info("ğŸ› ï¸ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Chroma...")
        chroma_client = chromadb.Client(Settings(
            persist_directory=CHROMA_DIR,
            is_persistent=True
        ))
        
        # 5. Ø§ÛŒØ¬Ø§Ø¯/Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ú©Ø§Ù„Ú©Ø´Ù†
        collection = chroma_client.get_or_create_collection(
            name="products",
            metadata={"hnsw:space": "cosine"}
        )
        
        # 6. ØªÙˆÙ„ÛŒØ¯ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯â€ŒÙ‡Ø§
        model = SentenceTransformer(MODEL_NAME)
        embeddings = model.encode(
            products_df['combined_text'].tolist(),
            show_progress_bar=True,
            batch_size=32
        )
        
        # 7. Ø§ÙØ²ÙˆØ¯Ù† Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ ChromaDB
        collection.add(
            embeddings=embeddings.tolist(),
            documents=products_df['combined_text'].tolist(),
            ids=[str(id) for id in products_df['Id'].tolist()]
        )
        
        logging.info(f"âœ… {collection.count()} Ù…Ø­ØµÙˆÙ„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø´Ø¯")

    except pyodbc.Error as e:
        logging.error(f"âŒ Ø®Ø·Ø§ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {str(e)}")
        raise
    except Exception as e:
        logging.error(f"âŒ Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡: {str(e)}")
        raise
    finally:
        if conn:
            conn.close()

# --- Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ÛŒ Ø³Ø±ÙˆØ± ---
@app.on_event("startup")
async def startup_event():
    try:
        load_data_and_init_chroma()
        logging.info("ğŸš€ Ø³Ø±ÙˆØ± Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯!")
    except Exception as e:
        logging.critical(f"ğŸ’¥ Ø®Ø·Ø§ÛŒ Ø¨Ø­Ø±Ø§Ù†ÛŒ: {str(e)}")
        sys.exit(1)

@app.on_event("shutdown")
async def shutdown_event():
    logging.info("ğŸ›‘ Ø¯Ø± Ø­Ø§Ù„ Ø®Ø§Ù…ÙˆØ´ Ú©Ø±Ø¯Ù† Ø³Ø±ÙˆØ±...")
    # Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ù†Ø§Ø¨Ø¹ ChromaDB
    if chroma_client:
        try:
            chroma_client.reset()  # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù…Ù†Ø§Ø¨Ø¹
        except Exception as e:
            logging.error(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ ChromaDB: {str(e)}")
    logging.info("âœ… Ù…Ù†Ø§Ø¨Ø¹ Ø¢Ø²Ø§Ø¯ Ø´Ø¯Ù†Ø¯")

# --- Ø§Ù†Ø¯Ù¾ÙˆÛŒÙ†Øªâ€ŒÙ‡Ø§ÛŒ API ---
# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† imports
from persian_spell_checker import PersianSpellChecker

# --- Ø§ÙØ²ÙˆØ¯Ù† ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¬Ø¯ÛŒØ¯ ---
SPELL_CHECKER = PersianSpellChecker()

# --- Ø§ØµÙ„Ø§Ø­ Ø§Ù†Ø¯Ù¾ÙˆÛŒÙ†Øª Ø¬Ø³ØªØ¬Ùˆ ---
@app.post("/search")
async def search_products(query: str, top_k: int = 5):
    try:
        # Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§
        if top_k < 1 or top_k > 100:
            return {"error": "Ù…Ù‚Ø¯Ø§Ø± top_k Ø¨Ø§ÛŒØ¯ Ø¨ÛŒÙ† Û± ØªØ§ Û±Û°Û° Ø¨Ø§Ø´Ø¯"}
        
        # ØªØµØ­ÛŒØ­ Ø®ÙˆØ¯Ú©Ø§Ø± Ø§Ù…Ù„Ø§ÛŒÛŒ
        corrected_query = SPELL_CHECKER.spell_corrector(query)
        if corrected_query != query:
            logging.info(f"Ø§ØµÙ„Ø§Ø­ Ø§Ù…Ù„Ø§ÛŒÛŒ: '{query}' => '{corrected_query}'")
            query = corrected_query
        
        # ØªÙ‚ÙˆÛŒØª Ø®ÙˆØ¯Ú©Ø§Ø± Ú©ÙˆØ¦Ø±ÛŒ
        query_booster = {
            "Ù…ÛŒÚ©Ø±ÙˆÙÙˆÙ†": ["ØµØ¯Ø§", "Ø¶Ø¨Ø·", "Ú©Ù¾Ú†Ø±", "Ø§Ø³Ù¾ÛŒÚ©Ø±"],
            "Ù„Ù¾ØªØ§Ù¾": ["Ù†ÙˆØªâ€ŒØ¨ÙˆÚ©", "Ø±Ø§ÛŒØ§Ù†Ù‡ Ù‡Ù…Ø±Ø§Ù‡", "Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±"],
            "Ù‡Ø¯ÙÙˆÙ†": ["Ù‡Ù†Ø¯Ø²ÙØ±ÛŒ", "Ø¨Ù„ÙˆØªÙˆØ«", "Ú¯ÙˆØ´ÛŒ"]
        }
        boosted_query = query.lower()
        for key, values in query_booster.items():
            if key in boosted_query:
                boosted_query += " " + " ".join(values)
        
        # ØªÙˆÙ„ÛŒØ¯ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯
        query_embedding = model.encode([boosted_query]).tolist()[0]
        
        # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± ChromaDB
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k * 5
        )
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†ØªØ§ÛŒØ¬
        final_results = []
        seen_ids = set()
        
        for idx, product_id in enumerate(results['ids'][0]):
            try:
                product = products_df[products_df['Id'] == int(product_id)].iloc[0]
                
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø²
                name_score = int(product['Name'].lower().count(query.lower())) * 3
                desc_score = int(product['Description'].lower().count(query.lower())) * 2
                kw_score = int(product['Keywords'].lower().count(query.lower())) * 1.5
                similarity_score = (name_score + desc_score + kw_score) * 0.8 + (1 - results['distances'][0][idx]) * 0.2
                
                if similarity_score > 0.1 and product['Id'] not in seen_ids:
                    final_results.append({
                        "id": int(product['Id']),
                        "name": str(product['Name']),
                        "score": float(round(similarity_score, 2)),
                        "matches": [
                            f"ØªØ·Ø§Ø¨Ù‚ Ø¯Ø± Ù†Ø§Ù… ({int(name_score)})",
                            f"ØªØ·Ø§Ø¨Ù‚ Ø¯Ø± ØªÙˆØ¶ÛŒØ­Ø§Øª ({int(desc_score)})"
                        ],
                        "original_query": query  # Ù†Ù…Ø§ÛŒØ´ Ú©ÙˆØ¦Ø±ÛŒ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡
                    })
                    seen_ids.add(int(product['Id']))
                
                if len(final_results) >= top_k:
                    break
                    
            except Exception as e:
                logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ø­ØµÙˆÙ„: {str(e)}")
        
        return {
            "results": final_results[:top_k],
            "corrected_query": corrected_query if corrected_query != query else None
        }
        
    except Exception as e:
        logging.error(f"Ø®Ø·Ø§: {str(e)}")
        return {"error": "Ø®Ø·Ø§ÛŒ Ø³Ø±ÙˆØ±"}
        
    

@app.get("/health")
async def health_check():
    return {
        "status": "ÙØ¹Ø§Ù„",
        "products_loaded": len(products_df) if products_df else 0,
        "chroma_status": "ÙØ¹Ø§Ù„" if collection else "ØºÛŒØ±ÙØ¹Ø§Ù„",
        "index_size": collection.count() if collection else 0
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        timeout_keep_alive=30,
        workers=1
    )